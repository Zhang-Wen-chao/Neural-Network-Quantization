{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the Model, Download Data and Model\n",
    "\n",
    " Download the data and unzip to data folder\n",
    "```bash\n",
    "mkdir data\n",
    "cd data\n",
    "wget https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-v1.zip\n",
    "unzip wikitext-2-v1.zip\n",
    "```\n",
    "\n",
    "Download model to the data folder:\n",
    "```bash\n",
    "wget https://s3.amazonaws.com/pytorch-tutorial-assets/word_language_model_quantize.pth\n",
    "```\n",
    "\n",
    "Define the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMModel(\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      "  (encoder): Embedding(33278, 512)\n",
      "  (rnn): LSTM(512, 256, num_layers=5, dropout=0.5)\n",
      "  (decoder): Linear(in_features=256, out_features=33278, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import os\n",
    "from io import open\n",
    "import time\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Model Definition\n",
    "class LSTMModel(nn.Module):\n",
    "    \"\"\"Container module with an encoder, a recurrent module, and a decoder.\"\"\"\n",
    "\n",
    "    def __init__(self, ntoken, ninp, nhid, nlayers, dropout=0.5):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.encoder = nn.Embedding(ntoken, ninp)\n",
    "        self.rnn = nn.LSTM(ninp, nhid, nlayers, dropout=dropout)\n",
    "        self.decoder = nn.Linear(nhid, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "        self.nhid = nhid\n",
    "        self.nlayers = nlayers\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        emb = self.drop(self.encoder(input))\n",
    "        output, hidden = self.rnn(emb, hidden)\n",
    "        output = self.drop(output)\n",
    "        decoded = self.decoder(output)\n",
    "        return decoded, hidden\n",
    "\n",
    "\n",
    "def init_hidden(lstm_model, bsz):\n",
    "    # get the weight tensor and create hidden layer in the same device\n",
    "    weight = lstm_model.encoder.weight\n",
    "    # get weight from quantized model\n",
    "    if not isinstance(weight, torch.Tensor):\n",
    "        weight = weight()\n",
    "    device = weight.device\n",
    "    nlayers = lstm_model.rnn.num_layers\n",
    "    nhid = lstm_model.rnn.hidden_size\n",
    "    return (torch.zeros(nlayers, bsz, nhid, device=device),\n",
    "            torch.zeros(nlayers, bsz, nhid, device=device))\n",
    "\n",
    "\n",
    "# Load Text Data\n",
    "class Dictionary(object):\n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = []\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2idx:\n",
    "            self.idx2word.append(word)\n",
    "            self.word2idx[word] = len(self.idx2word) - 1\n",
    "        return self.word2idx[word]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx2word)\n",
    "\n",
    "\n",
    "class Corpus(object):\n",
    "    def __init__(self, path):\n",
    "        self.dictionary = Dictionary()\n",
    "        self.train = self.tokenize(os.path.join(path, 'wiki.train.tokens'))\n",
    "        self.valid = self.tokenize(os.path.join(path, 'wiki.valid.tokens'))\n",
    "        self.test = self.tokenize(os.path.join(path, 'wiki.test.tokens'))\n",
    "\n",
    "    def tokenize(self, path):\n",
    "        \"\"\"Tokenizes a text file.\"\"\"\n",
    "        assert os.path.exists(path)\n",
    "        # Add words to the dictionary\n",
    "        with open(path, 'r', encoding=\"utf8\") as f:\n",
    "            for line in f:\n",
    "                words = line.split() + ['<eos>']\n",
    "                for word in words:\n",
    "                    self.dictionary.add_word(word)\n",
    "\n",
    "        # Tokenize file content\n",
    "        with open(path, 'r', encoding=\"utf8\") as f:\n",
    "            idss = []\n",
    "            for line in f:\n",
    "                words = line.split() + ['<eos>']\n",
    "                ids = []\n",
    "                for word in words:\n",
    "                    ids.append(self.dictionary.word2idx[word])\n",
    "                idss.append(torch.tensor(ids).type(torch.int64))\n",
    "            ids = torch.cat(idss)\n",
    "\n",
    "        return ids\n",
    "\n",
    "model_data_filepath = 'data/'\n",
    "\n",
    "corpus = Corpus(model_data_filepath + 'wikitext-2')\n",
    "\n",
    "ntokens = len(corpus.dictionary)\n",
    "\n",
    "# Load Pretrained Model\n",
    "model = LSTMModel(\n",
    "    ntoken = ntokens,\n",
    "    ninp = 512,\n",
    "    nhid = 256,\n",
    "    nlayers = 5,\n",
    ")\n",
    "\n",
    "model.load_state_dict(\n",
    "    torch.load(\n",
    "        model_data_filepath + 'word_language_model_quantize.pth',\n",
    "        map_location=torch.device('cpu')\n",
    "        )\n",
    "    )\n",
    "\n",
    "model.eval()\n",
    "print(model)\n",
    "\n",
    "bptt = 25\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "eval_batch_size = 1\n",
    "\n",
    "# create test data set\n",
    "def batchify(data, bsz):\n",
    "    # Work out how cleanly we can divide the dataset into bsz parts.\n",
    "    nbatch = data.size(0) // bsz\n",
    "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
    "    data = data.narrow(0, 0, nbatch * bsz)\n",
    "    # Evenly divide the data across the bsz batches.\n",
    "    return data.view(bsz, -1).t().contiguous()\n",
    "\n",
    "test_data = batchify(corpus.test, eval_batch_size)\n",
    "example_inputs = (next(iter(test_data))[0])\n",
    "\n",
    "# Evaluation functions\n",
    "def get_batch(source, i):\n",
    "    seq_len = min(bptt, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len].reshape(-1)\n",
    "    return data, target\n",
    "\n",
    "def repackage_hidden(h):\n",
    "  \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n",
    "\n",
    "  if isinstance(h, torch.Tensor):\n",
    "      return h.detach()\n",
    "  else:\n",
    "      return tuple(repackage_hidden(v) for v in h)\n",
    "\n",
    "def evaluate(model_, data_source):\n",
    "    # Turn on evaluation mode which disables dropout.\n",
    "    model_.eval()\n",
    "    total_loss = 0.\n",
    "    hidden = init_hidden(model_, eval_batch_size)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, data_source.size(0) - 1, bptt):\n",
    "            data, targets = get_batch(data_source, i)\n",
    "            output, hidden = model_(data, hidden)\n",
    "            hidden = repackage_hidden(hidden)\n",
    "            output_flat = output.view(-1, ntokens)\n",
    "            total_loss += len(data) * criterion(output_flat, targets).item()\n",
    "    return total_loss / (len(data_source) - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post Training Dynamic Quantization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prepared model: GraphModule(\n",
      "  (encoder): Embedding(33278, 512)\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      "  (activation_post_process_0): PlaceholderObserver(dtype=torch.quint8, is_dynamic=True)\n",
      "  (rnn): LSTM(512, 256, num_layers=5, dropout=0.5)\n",
      "  (activation_post_process_1): PlaceholderObserver(dtype=torch.quint8, is_dynamic=True)\n",
      "  (decoder): Linear(in_features=256, out_features=33278, bias=True)\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, input, hidden):\n",
      "    input_1 = input\n",
      "    encoder = self.encoder(input_1);  input_1 = None\n",
      "    drop = self.drop(encoder);  encoder = None\n",
      "    activation_post_process_0 = self.activation_post_process_0(drop);  drop = None\n",
      "    rnn = self.rnn(activation_post_process_0, hidden);  activation_post_process_0 = hidden = None\n",
      "    getitem = rnn[0]\n",
      "    getitem_1 = rnn[1];  rnn = None\n",
      "    drop_1 = self.drop(getitem);  getitem = None\n",
      "    activation_post_process_1 = self.activation_post_process_1(drop_1);  drop_1 = None\n",
      "    decoder = self.decoder(activation_post_process_1);  activation_post_process_1 = None\n",
      "    return (decoder, getitem_1)\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student001/anaconda3/lib/python3.9/site-packages/torch/ao/nn/quantized/reference/modules/rnn.py:320: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(weight_qparams[\"scale\"], dtype=torch.float, device=device))\n",
      "/home/student001/anaconda3/lib/python3.9/site-packages/torch/ao/nn/quantized/reference/modules/rnn.py:323: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(weight_qparams[\"zero_point\"], dtype=torch.int, device=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quantized model GraphModule(\n",
      "  (encoder): QuantizedEmbedding(num_embeddings=33278, embedding_dim=512, dtype=torch.quint8, qscheme=torch.per_channel_affine_float_qparams)\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      "  (rnn): DynamicQuantizedLSTM(512, 256, num_layers=5, dropout=0.5)\n",
      "  (decoder): DynamicQuantizedLinear(in_features=256, out_features=33278, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, input, hidden):\n",
      "    input_1 = input\n",
      "    encoder = self.encoder(input_1);  input_1 = None\n",
      "    drop = self.drop(encoder);  encoder = None\n",
      "    rnn = self.rnn(drop, hidden);  drop = hidden = None\n",
      "    getitem = rnn[0]\n",
      "    getitem_1 = rnn[1];  rnn = None\n",
      "    drop_1 = self.drop(getitem);  getitem = None\n",
      "    decoder = self.decoder(drop_1);  drop_1 = None\n",
      "    return (decoder, getitem_1)\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n"
     ]
    }
   ],
   "source": [
    "from torch.quantization.quantize_fx import prepare_fx, convert_fx\n",
    "from torch.ao.quantization import default_dynamic_qconfig, float_qparams_weight_only_qconfig, QConfigMapping\n",
    "\n",
    "# Full docs for supported qconfig for floating point modules/ops can be found in `quantization docs <https://pytorch.org/docs/stable/quantization.html#module-torch.quantization>`_\n",
    "# Full docs for `QConfigMapping <https://pytorch.org/docs/stable/generated/torch.ao.quantization.qconfig_mapping.QConfigMapping.html#torch.ao.quantization.qconfig_mapping.QConfigMapping>`_\n",
    "qconfig_mapping = (QConfigMapping()\n",
    "    .set_object_type(nn.Embedding, float_qparams_weight_only_qconfig)\n",
    "    .set_object_type(nn.LSTM, default_dynamic_qconfig)\n",
    "    .set_object_type(nn.Linear, default_dynamic_qconfig)\n",
    ")\n",
    "# Load model to create the original model because quantization api changes the model inplace and we want\n",
    "# to keep the original model for future comparison\n",
    "\n",
    "\n",
    "model_to_quantize = LSTMModel(\n",
    "    ntoken = ntokens,\n",
    "    ninp = 512,\n",
    "    nhid = 256,\n",
    "    nlayers = 5,\n",
    ")\n",
    "\n",
    "model_to_quantize.load_state_dict(\n",
    "    torch.load(\n",
    "        model_data_filepath + 'word_language_model_quantize.pth',\n",
    "        map_location=torch.device('cpu')\n",
    "        )\n",
    "    )\n",
    "\n",
    "model_to_quantize.eval()\n",
    "\n",
    "\n",
    "prepared_model = prepare_fx(model_to_quantize, qconfig_mapping, example_inputs)\n",
    "print(\"prepared model:\", prepared_model)\n",
    "quantized_model = convert_fx(prepared_model)\n",
    "print(\"quantized model\", quantized_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size (MB): 113.944064\n",
      "Size (MB): 28.890344\n"
     ]
    }
   ],
   "source": [
    "def print_size_of_model(model):\n",
    "    torch.save(model.state_dict(), \"temp.p\")\n",
    "    print('Size (MB):', os.path.getsize(\"temp.p\")/1e6)\n",
    "    os.remove('temp.p')\n",
    "\n",
    "print_size_of_model(model)\n",
    "print_size_of_model(quantized_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 5.167\n",
      "elapsed time (seconds): 144.2\n",
      "loss: 5.167\n",
      "elapsed time (seconds): 64.0\n"
     ]
    }
   ],
   "source": [
    "torch.set_num_threads(1)\n",
    "\n",
    "def time_model_evaluation(model, test_data):\n",
    "    s = time.time()\n",
    "    loss = evaluate(model, test_data)\n",
    "    elapsed = time.time() - s\n",
    "    print('''loss: {0:.3f}\\nelapsed time (seconds): {1:.1f}'''.format(loss, elapsed))\n",
    "\n",
    "time_model_evaluation(model, test_data)\n",
    "time_model_evaluation(quantized_model, test_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
