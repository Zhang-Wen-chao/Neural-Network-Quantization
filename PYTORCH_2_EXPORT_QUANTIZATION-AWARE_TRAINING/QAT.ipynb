{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student001/anaconda3/lib/python3.9/site-packages/torch/ao/quantization/observer.py:1207: UserWarning: must run observer before calling calculate_qparams.                                    Returning default scale and zero point \n",
      "  warnings.warn(\n",
      "/home/student001/anaconda3/lib/python3.9/site-packages/torch/ao/quantization/utils.py:317: UserWarning: must run observer before calling calculate_qparams. Returning default values.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GraphModule()"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch._export import capture_pre_autograd_graph\n",
    "from torch.ao.quantization.quantize_pt2e import (\n",
    "  prepare_qat_pt2e,\n",
    "  convert_pt2e,\n",
    ")\n",
    "# from torch.ao.quantization.quantizer import (\n",
    "from torch.ao.quantization.quantizer.xnnpack_quantizer import (\n",
    "  XNNPACKQuantizer,\n",
    "  get_symmetric_quantization_config,\n",
    ")\n",
    "\n",
    "class M(torch.nn.Module):\n",
    "   def __init__(self):\n",
    "      super().__init__()\n",
    "      self.linear = torch.nn.Linear(5, 10)\n",
    "\n",
    "   def forward(self, x):\n",
    "      return self.linear(x)\n",
    "\n",
    "\n",
    "example_inputs = (torch.randn(1, 5),)\n",
    "m = M()\n",
    "\n",
    "# Step 1. program capture\n",
    "# NOTE: this API will be updated to torch.export API in the future, but the captured\n",
    "# result shoud mostly stay the same\n",
    "m = capture_pre_autograd_graph(m, *example_inputs)\n",
    "# we get a model with aten ops\n",
    "\n",
    "# Step 2. quantization-aware training\n",
    "# backend developer will write their own Quantizer and expose methods to allow\n",
    "# users to express how they want the model to be quantized\n",
    "quantizer = XNNPACKQuantizer().set_global(get_symmetric_quantization_config())\n",
    "m = prepare_qat_pt2e(m, quantizer)\n",
    "\n",
    "# train omitted\n",
    "\n",
    "m = convert_pt2e(m)\n",
    "# we have a model with aten ops doing integer computations when possible\n",
    "\n",
    "# move the quantized model to eval mode, equivalent to `m.eval()`\n",
    "torch.ao.quantization.move_exported_model_to_eval(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class GraphModule(torch.nn.Module):\n",
      "    def forward(self, x):\n",
      "        arg0: f32[5], = fx_pytree.tree_flatten_spec(([x], {}), self._in_spec)\n",
      "        # No stacktrace found for following nodes\n",
      "        quantize_per_tensor_default = torch.ops.quantized_decomposed.quantize_per_tensor.default(arg0, 1.0, 0, -128, 127, torch.int8);  arg0 = None\n",
      "        dequantize_per_tensor_default = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default, 1.0, 0, -128, 127, torch.int8);  quantize_per_tensor_default = None\n",
      "        \n",
      "        # File: /tmp/ipykernel_1051559/4076165061.py:19, code: return self.linear(x)\n",
      "        _param_constant0 = self._param_constant0\n",
      "        \n",
      "        # No stacktrace found for following nodes\n",
      "        quantize_per_tensor_default_1 = torch.ops.quantized_decomposed.quantize_per_tensor.default(_param_constant0, 1.0, 0, -127, 127, torch.int8);  _param_constant0 = None\n",
      "        dequantize_per_tensor_default_1 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_1, 1.0, 0, -127, 127, torch.int8);  quantize_per_tensor_default_1 = None\n",
      "        \n",
      "        # File: /tmp/ipykernel_1051559/4076165061.py:19, code: return self.linear(x)\n",
      "        _param_constant1 = self._param_constant1\n",
      "        linear_default: f32[10] = torch.ops.aten.linear.default(dequantize_per_tensor_default, dequantize_per_tensor_default_1, _param_constant1);  dequantize_per_tensor_default = dequantize_per_tensor_default_1 = _param_constant1 = None\n",
      "        \n",
      "        # No stacktrace found for following nodes\n",
      "        quantize_per_tensor_default_2 = torch.ops.quantized_decomposed.quantize_per_tensor.default(linear_default, 1.0, 0, -128, 127, torch.int8);  linear_default = None\n",
      "        dequantize_per_tensor_default_2 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_2, 1.0, 0, -128, 127, torch.int8);  quantize_per_tensor_default_2 = None\n",
      "        return pytree.tree_unflatten([dequantize_per_tensor_default_2], self._out_spec)\n",
      "        \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'class GraphModule(torch.nn.Module):\\n    def forward(self, x):\\n        arg0: f32[5], = fx_pytree.tree_flatten_spec(([x], {}), self._in_spec)\\n        # No stacktrace found for following nodes\\n        quantize_per_tensor_default = torch.ops.quantized_decomposed.quantize_per_tensor.default(arg0, 1.0, 0, -128, 127, torch.int8);  arg0 = None\\n        dequantize_per_tensor_default = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default, 1.0, 0, -128, 127, torch.int8);  quantize_per_tensor_default = None\\n        \\n        # File: /tmp/ipykernel_1051559/4076165061.py:19, code: return self.linear(x)\\n        _param_constant0 = self._param_constant0\\n        \\n        # No stacktrace found for following nodes\\n        quantize_per_tensor_default_1 = torch.ops.quantized_decomposed.quantize_per_tensor.default(_param_constant0, 1.0, 0, -127, 127, torch.int8);  _param_constant0 = None\\n        dequantize_per_tensor_default_1 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_1, 1.0, 0, -127, 127, torch.int8);  quantize_per_tensor_default_1 = None\\n        \\n        # File: /tmp/ipykernel_1051559/4076165061.py:19, code: return self.linear(x)\\n        _param_constant1 = self._param_constant1\\n        linear_default: f32[10] = torch.ops.aten.linear.default(dequantize_per_tensor_default, dequantize_per_tensor_default_1, _param_constant1);  dequantize_per_tensor_default = dequantize_per_tensor_default_1 = _param_constant1 = None\\n        \\n        # No stacktrace found for following nodes\\n        quantize_per_tensor_default_2 = torch.ops.quantized_decomposed.quantize_per_tensor.default(linear_default, 1.0, 0, -128, 127, torch.int8);  linear_default = None\\n        dequantize_per_tensor_default_2 = torch.ops.quantized_decomposed.dequantize_per_tensor.default(quantize_per_tensor_default_2, 1.0, 0, -128, 127, torch.int8);  quantize_per_tensor_default_2 = None\\n        return pytree.tree_unflatten([dequantize_per_tensor_default_2], self._out_spec)\\n        '"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.print_readable()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
